import os
import sys
import argparse
import logging
from datetime import datetime
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from dotenv import load_dotenv
load_dotenv()

# Imports des modules r√©volutionnaires
from covid_data_pipeline import IntelligentCovidDataPipeline
from covid_ai_model import CovidRevolutionaryTrainer

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('revolutionary_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RevolutionaryTrainingOrchestrator:
    """Orchestrateur pour l'entra√Ænement r√©volutionnaire complet"""
    
    def __init__(self, config: dict):
        self.config = config
        self.pipeline = None
        self.trainer = None
        self.enriched_data = None
        
        # Cr√©er les dossiers n√©cessaires
        os.makedirs('models', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        os.makedirs('outputs', exist_ok=True)
        
        logger.info("üöÄ Orchestrateur r√©volutionnaire initialis√©")
    
    def validate_environment(self) -> bool:
        """Valide l'environnement d'entra√Ænement"""
        logger.info("üîç Validation de l'environnement...")
        
        required_packages = [
            'torch', 'pandas', 'numpy', 'sklearn', 'pymongo', 
            'matplotlib', 'seaborn', 'fastapi', 'uvicorn'
        ]
        
        missing_packages = []
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                missing_packages.append(package)
        
        if missing_packages:
            logger.error(f"‚ùå Packages manquants: {missing_packages}")
            return False
        
        # V√©rifier les fichiers de donn√©es
        csv_path = Path(self.config['csv_data_path'])
        required_files = [
            'cumulative-covid-vaccinations_clean.csv',
            'consolidated_demographics_data.csv'
        ]
        
        missing_files = []
        for file in required_files:
            if not (csv_path / file).exists():
                missing_files.append(file)
        
        if missing_files:
            logger.error(f"‚ùå Fichiers CSV manquants: {missing_files}")
            return False
        
        logger.info("‚úÖ Environnement valid√©")
        return True
    
    def run_data_pipeline(self) -> pd.DataFrame:
        """Ex√©cute le pipeline de donn√©es intelligent"""
        logger.info("üìä √âTAPE 1: PIPELINE DE DONN√âES")
        
        self.pipeline = IntelligentCovidDataPipeline(
            mongo_uri=self.config['mongo_uri'],
            db_name=self.config['db_name'],
            csv_data_path=self.config['csv_data_path']
        )
        
        try:
            self.enriched_data = self.pipeline.run_full_pipeline()
            
            # Statistiques du dataset
            logger.info("üìà STATISTIQUES DU DATASET ENRICHI:")
            logger.info(f"   Lignes: {len(self.enriched_data):,}")
            logger.info(f"   Features: {len(self.enriched_data.columns)}")
            logger.info(f"   Pays: {self.enriched_data['country_name'].nunique()}")
            logger.info(f"   P√©riode: {self.enriched_data['date'].min()} ‚Üí {self.enriched_data['date'].max()}")
            
            # Analyse de la qualit√© des donn√©es
            self.analyze_data_quality()
            
            return self.enriched_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur pipeline donn√©es: {e}")
            raise
    
    def analyze_data_quality(self):
        """Analyse la qualit√© des donn√©es enrichies"""
        logger.info("üîç Analyse de la qualit√© des donn√©es...")
        
        if self.enriched_data is None:
            return
        
        # Analyse des valeurs manquantes
        missing_analysis = self.enriched_data.isnull().sum()
        missing_pct = (missing_analysis / len(self.enriched_data)) * 100
        
        # Features avec beaucoup de valeurs manquantes
        problematic_features = missing_pct[missing_pct > 20].sort_values(ascending=False)
        
        if len(problematic_features) > 0:
            logger.warning(f"‚ö†Ô∏è Features avec >20% valeurs manquantes:")
            for feature, pct in problematic_features.items():
                logger.warning(f"   {feature}: {pct:.1f}%")
        
        # Analyse par pays
        countries_data_count = self.enriched_data['country_name'].value_counts()
        logger.info(f"üìä Pays avec le plus de donn√©es: {countries_data_count.head().to_dict()}")
        logger.info(f"üìä Pays avec le moins de donn√©es: {countries_data_count.tail().to_dict()}")
        
        # D√©tection d'outliers simples
        numeric_columns = self.enriched_data.select_dtypes(include=[np.number]).columns
        outliers_summary = {}
        
        for col in numeric_columns:
            Q1 = self.enriched_data[col].quantile(0.25)
            Q3 = self.enriched_data[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = ((self.enriched_data[col] < (Q1 - 1.5 * IQR)) | 
                       (self.enriched_data[col] > (Q3 + 1.5 * IQR))).sum()
            if outliers > 0:
                outliers_summary[col] = outliers
        
        if outliers_summary:
            top_outliers = sorted(outliers_summary.items(), key=lambda x: x[1], reverse=True)[:5]
            logger.info(f"üìä Features avec le plus d'outliers: {dict(top_outliers)}")
        
        # Sauvegarde du rapport
        self.save_data_quality_report(missing_pct, countries_data_count, outliers_summary)
    
    def save_data_quality_report(self, missing_pct, countries_data_count, outliers_summary):
        """Sauvegarde un rapport de qualit√© des donn√©es"""
        try:
            # Graphiques de qualit√©
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # 1. Valeurs manquantes
            top_missing = missing_pct[missing_pct > 0].head(15)
            if len(top_missing) > 0:
                axes[0,0].barh(range(len(top_missing)), top_missing.values)
                axes[0,0].set_yticks(range(len(top_missing)))
                axes[0,0].set_yticklabels(top_missing.index, fontsize=8)
                axes[0,0].set_xlabel('Pourcentage de valeurs manquantes')
                axes[0,0].set_title('Top 15 Features avec Valeurs Manquantes')
            
            # 2. Distribution des donn√©es par pays
            top_countries = countries_data_count.head(10)
            axes[0,1].bar(range(len(top_countries)), top_countries.values)
            axes[0,1].set_xticks(range(len(top_countries)))
            axes[0,1].set_xticklabels(top_countries.index, rotation=45, fontsize=8)
            axes[0,1].set_ylabel('Nombre de points de donn√©es')
            axes[0,1].set_title('Top 10 Pays par Volume de Donn√©es')
            
            # 3. √âvolution temporelle
            temporal_data = self.enriched_data.groupby('date').size()
            axes[1,0].plot(temporal_data.index, temporal_data.values)
            axes[1,0].set_xlabel('Date')
            axes[1,0].set_ylabel('Nombre de points de donn√©es')
            axes[1,0].set_title('√âvolution Temporelle des Donn√©es')
            axes[1,0].tick_params(axis='x', rotation=45)
            
            # 4. Outliers
            if outliers_summary:
                top_outliers = sorted(outliers_summary.items(), key=lambda x: x[1], reverse=True)[:10]
                outlier_names, outlier_counts = zip(*top_outliers)
                axes[1,1].barh(range(len(outlier_names)), outlier_counts)
                axes[1,1].set_yticks(range(len(outlier_names)))
                axes[1,1].set_yticklabels(outlier_names, fontsize=8)
                axes[1,1].set_xlabel('Nombre d\'outliers')
                axes[1,1].set_title('Top 10 Features avec Outliers')
            
            plt.tight_layout()
            plt.savefig('outputs/data_quality_report.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("‚úÖ Rapport de qualit√© sauvegard√©: outputs/data_quality_report.png")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde rapport qualit√©: {e}")
    
    def run_model_training(self):
        """Lance l'entra√Ænement du mod√®le r√©volutionnaire"""
        logger.info("üß† √âTAPE 2: ENTRA√éNEMENT DU MOD√àLE R√âVOLUTIONNAIRE")
        
        if self.enriched_data is None:
            raise ValueError("Donn√©es enrichies non disponibles. Lancez d'abord le pipeline de donn√©es.")
        
        # Configuration du mod√®le
        model_config = self.config.get('model_config', {
            'd_model': 256,
            'n_heads': 8,
            'n_layers': 6,
            'd_ff': 1024,
            'dropout': 0.1,
            'prediction_horizons': [1, 7, 14, 30]
        })
        
        self.trainer = CovidRevolutionaryTrainer(model_config)
        
        try:
            # Pr√©paration des donn√©es
            logger.info("üéØ Pr√©paration des donn√©es pour l'entra√Ænement...")
            sequences, static_features, targets = self.trainer.prepare_revolutionary_dataset(
                self.enriched_data, 
                sequence_length=self.config.get('sequence_length', 30)
            )
            
            # Cr√©ation des DataLoaders
            train_loader, val_loader = self.trainer.create_dataloaders(
                sequences, static_features, targets,
                batch_size=self.config.get('batch_size', 32),
                val_split=self.config.get('val_split', 0.2)
            )
            
            # Entra√Ænement
            logger.info("üöÄ D√©marrage de l'entra√Ænement r√©volutionnaire...")
            history = self.trainer.train_revolutionary_model(
                train_loader, val_loader,
                epochs=self.config.get('epochs', 100),
                learning_rate=self.config.get('learning_rate', 1e-4)
            )
            
            # Sauvegarde des artefacts
            self.trainer.save_artifacts(history)
            
            # √âvaluation finale
            self.evaluate_model_performance(val_loader, history)
            
            return history
            
        except Exception as e:
            logger.error(f"‚ùå Erreur entra√Ænement: {e}")
            raise
    
    def evaluate_model_performance(self, val_loader, history):
        """√âvalue les performances finales du mod√®le"""
        logger.info("üìä √âVALUATION FINALE DU MOD√àLE")
        
        try:
            # M√©triques d'entra√Ænement
            if history and 'val_metrics' in history:
                final_metrics = history['val_metrics'][-1] if history['val_metrics'] else {}
                
                logger.info("üéØ M√âTRIQUES FINALES:")
                for metric_name, value in final_metrics.items():
                    if isinstance(value, (int, float)):
                        logger.info(f"   {metric_name}: {value:.4f}")
            
            # Test sur quelques pr√©dictions
            self.test_sample_predictions()
            
        except Exception as e:
            logger.error(f"‚ùå Erreur √©valuation: {e}")
    
    def test_sample_predictions(self):
        """Teste le mod√®le sur quelques pr√©dictions √©chantillons"""
        logger.info("üß™ Test de pr√©dictions √©chantillons...")
        
        try:
            # S√©lectionner quelques pays pour test
            test_countries = ['France', 'Germany', 'Italy', 'Spain', 'United Kingdom']
            available_countries = self.enriched_data['country_name'].unique()
            
            test_countries = [c for c in test_countries if c in available_countries][:3]
            
            if not test_countries:
                test_countries = list(available_countries)[:3]
            
            logger.info(f"üß™ Test sur les pays: {test_countries}")
            
            # Simulations de pr√©dictions (logique simplifi√©e)
            for country in test_countries:
                country_data = self.enriched_data[self.enriched_data['country_name'] == country]
                if len(country_data) > 30:
                    latest_data = country_data.tail(1).iloc[0]
                    logger.info(f"   {country}: Derni√®res donn√©es - "
                              f"Confirm√©s: {latest_data.get('confirmed', 0):,.0f}, "
                              f"D√©c√®s: {latest_data.get('deaths', 0):,.0f}")
        
        except Exception as e:
            logger.error(f"‚ùå Erreur test pr√©dictions: {e}")
    
    def generate_final_report(self, history):
        """G√©n√®re un rapport final complet"""
        logger.info("üìù G√âN√âRATION DU RAPPORT FINAL")
        
        try:
            report = {
                "training_summary": {
                    "model_type": "COVID Revolutionary Transformer v2.0",
                    "training_date": datetime.now().isoformat(),
                    "dataset_size": len(self.enriched_data) if self.enriched_data is not None else 0,
                    "countries_count": self.enriched_data['country_name'].nunique() if self.enriched_data is not None else 0,
                    "features_count": len(self.enriched_data.columns) if self.enriched_data is not None else 0,
                    "epochs_completed": len(history.get('train_loss', [])) if history else 0
                },
                "data_sources": {
                    "covid_timeseries": "MongoDB",
                    "vaccination_data": "cumulative-covid-vaccinations_clean.csv",
                    "demographics": "consolidated_demographics_data.csv"
                },
                "model_architecture": self.config.get('model_config', {}),
                "training_config": {
                    k: v for k, v in self.config.items() 
                    if k not in ['mongo_uri', 'db_name']
                },
                "final_performance": history.get('val_metrics', [])[-1] if history and history.get('val_metrics') else {}
            }
            
            # Sauvegarde du rapport
            import json
            with open('outputs/final_training_report.json', 'w') as f:
                json.dump(report, f, indent=2, default=str)
            
            logger.info("‚úÖ Rapport final sauvegard√©: outputs/final_training_report.json")
            
            # Affichage du r√©sum√©
            logger.info("üéâ R√âSUM√â DE L'ENTRA√éNEMENT R√âVOLUTIONNAIRE:")
            logger.info(f"   üìä Dataset: {report['training_summary']['dataset_size']:,} lignes")
            logger.info(f"   üè≥Ô∏è Pays: {report['training_summary']['countries_count']}")
            logger.info(f"   üìà Features: {report['training_summary']['features_count']}")
            logger.info(f"   üîÑ Epochs: {report['training_summary']['epochs_completed']}")
            
            if report['final_performance']:
                logger.info("   üéØ Performance finale:")
                for metric, value in report['final_performance'].items():
                    if isinstance(value, (int, float)):
                        logger.info(f"      {metric}: {value:.4f}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration rapport: {e}")
    
    def run_complete_training(self):
        """Lance l'entra√Ænement complet r√©volutionnaire"""
        logger.info("üöÄ D√âMARRAGE DE L'ENTRA√éNEMENT R√âVOLUTIONNAIRE COMPLET")
        logger.info("=" * 80)
        
        try:
            # 1. Validation environnement
            if not self.validate_environment():
                raise ValueError("Environnement non valide")
            
            # 2. Pipeline de donn√©es
            self.run_data_pipeline()
            
            # 3. Entra√Ænement du mod√®le
            history = self.run_model_training()
            
            # 4. Rapport final
            self.generate_final_report(history)
            
            logger.info("=" * 80)
            logger.info("üéâ ENTRA√éNEMENT R√âVOLUTIONNAIRE TERMIN√â AVEC SUCC√àS!")
            logger.info("üìÅ Fichiers g√©n√©r√©s:")
            logger.info("   - models/covid_revolutionary_model.pth")
            logger.info("   - models/revolutionary_*_scaler.pkl")
            logger.info("   - models/revolutionary_config.json")
            logger.info("   - outputs/data_quality_report.png")
            logger.info("   - outputs/final_training_report.json")
            logger.info("   - models/training_history.png")
            logger.info("\nüöÄ Le mod√®le r√©volutionnaire est pr√™t pour l'API!")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå √âCHEC DE L'ENTRA√éNEMENT: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """Point d'entr√©e principal"""
    parser = argparse.ArgumentParser(description="Entra√Ænement r√©volutionnaire COVID IA v2.0")
    parser.add_argument("--config", type=str, help="Fichier de configuration JSON")
    parser.add_argument("--mongo-uri", type=str, default="mongodb://localhost:27017", help="URI MongoDB")
    parser.add_argument("--db-name", type=str, default="covid_dashboard", help="Nom de la base de donn√©es")
    parser.add_argument("--csv-path", type=str, default="../data/dataset_clean", help="Chemin des fichiers CSV")
    parser.add_argument("--epochs", type=int, default=100, help="Nombre d'epochs")
    parser.add_argument("--batch-size", type=int, default=32, help="Taille du batch")
    parser.add_argument("--learning-rate", type=float, default=1e-4, help="Taux d'apprentissage")
    
    args = parser.parse_args()
    
    # Charger variables d'environnement
    load_dotenv()
    
    # Configuration par d√©faut
    config = {
        'mongo_uri': args.mongo_uri or os.getenv('MONGO_URI'),
        'db_name': args.db_name or os.getenv('DB_NAME'),
        'csv_data_path': args.csv_path,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'sequence_length': 30,
        'val_split': 0.2,
        'model_config': {
            'd_model': 256,
            'n_heads': 8,
            'n_layers': 6,
            'd_ff': 1024,
            'dropout': 0.1,
            'prediction_horizons': [1, 7, 14, 30]
        }
    }
    
    # Charger configuration depuis fichier si sp√©cifi√©
    if args.config and os.path.exists(args.config):
        import json
        with open(args.config, 'r') as f:
            file_config = json.load(f)
            config.update(file_config)
        logger.info(f"‚úÖ Configuration charg√©e depuis {args.config}")
    
    # Lancer l'entra√Ænement
    orchestrator = RevolutionaryTrainingOrchestrator(config)
    success = orchestrator.run_complete_training()
    
    if success:
        print("\n" + "="*50)
        print("üéâ SUCC√àS! Le mod√®le r√©volutionnaire est pr√™t!")
        print("üìö Prochaines √©tapes:")
        print("   1. Lancer l'API: python covid_revolutionary_api.py")
        print("   2. Tester les pr√©dictions: /predict endpoint")
        print("   3. Int√©grer avec le dashboard Vue.js")
        print("="*50)
        sys.exit(0)
    else:
        print("\n‚ùå √âCHEC de l'entra√Ænement. V√©rifiez les logs.")
        sys.exit(1)

if __name__ == "__main__":
    main()