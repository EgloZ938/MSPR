import os
import sys
import argparse
import logging
from datetime import datetime
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from dotenv import load_dotenv
load_dotenv()

# Imports des modules rÃ©volutionnaires (version CSV)
from covid_data_pipeline import CSVCovidDataPipeline
from covid_ai_model import CovidRevolutionaryTrainer

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('csv_revolutionary_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class CSVRevolutionaryTrainingOrchestrator:
    """ğŸš€ Orchestrateur pour l'entraÃ®nement rÃ©volutionnaire 100% CSV"""
    
    def __init__(self, config: dict):
        self.config = config
        self.pipeline = None
        self.trainer = None
        self.enriched_data = None
        
        # CrÃ©er les dossiers nÃ©cessaires
        os.makedirs('models', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        os.makedirs('outputs', exist_ok=True)
        
        logger.info("ğŸš€ Orchestrateur CSV RÃ©volutionnaire initialisÃ©")
    
    def validate_csv_environment(self) -> bool:
        """Valide l'environnement CSV"""
        logger.info("ğŸ” Validation de l'environnement CSV...")
        
        # VÃ©rifier les packages Python
        required_packages = [
            'torch', 'pandas', 'numpy', 'sklearn', 
            'matplotlib', 'seaborn', 'fastapi', 'uvicorn'
        ]
        
        missing_packages = []
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                missing_packages.append(package)
        
        if missing_packages:
            logger.error(f"âŒ Packages manquants: {missing_packages}")
            return False
        
        # VÃ©rifier les fichiers CSV
        csv_path = Path(self.config['csv_data_path'])
        required_files = [
            'covid_19_clean_complete_clean.csv',  # Principal
            'full_grouped_clean.csv',             # Alternatif
            'cumulative-covid-vaccinations_clean.csv',
            'consolidated_demographics_data.csv'
        ]
        
        # Au moins un fichier COVID principal doit exister
        covid_files = [
            csv_path / 'covid_19_clean_complete_clean.csv',
            csv_path / 'full_grouped_clean.csv'
        ]
        
        covid_found = any(f.exists() for f in covid_files)
        if not covid_found:
            logger.error("âŒ Aucun fichier COVID principal trouvÃ©!")
            logger.error(f"   CherchÃ© dans: {covid_files}")
            return False
        
        # VÃ©rifier les autres fichiers
        missing_files = []
        for file in required_files[2:]:  # Skip COVID files dÃ©jÃ  vÃ©rifiÃ©s
            if not (csv_path / file).exists():
                missing_files.append(file)
        
        if missing_files:
            logger.warning(f"âš ï¸ Fichiers CSV optionnels manquants: {missing_files}")
            logger.warning("   Le pipeline utilisera des valeurs par dÃ©faut")
        
        logger.info("âœ… Environnement CSV validÃ©")
        return True
    
    def run_csv_data_pipeline(self) -> pd.DataFrame:
        """ğŸš€ ExÃ©cute le pipeline de donnÃ©es CSV"""
        logger.info("ğŸ“Š Ã‰TAPE 1: PIPELINE DE DONNÃ‰ES CSV")
        
        self.pipeline = CSVCovidDataPipeline(
            csv_data_path=self.config['csv_data_path']
        )
        
        try:
            self.enriched_data = self.pipeline.run_full_pipeline()
            
            # Statistiques du dataset
            logger.info("ğŸ“ˆ STATISTIQUES DU DATASET CSV ENRICHI:")
            logger.info(f"   Lignes: {len(self.enriched_data):,}")
            logger.info(f"   Features: {len(self.enriched_data.columns)}")
            logger.info(f"   Pays: {self.enriched_data['country_name'].nunique()}")
            logger.info(f"   PÃ©riode: {self.enriched_data['date'].min()} â†’ {self.enriched_data['date'].max()}")
            
            # Analyse de la qualitÃ© des donnÃ©es
            self.analyze_csv_data_quality()
            
            return self.enriched_data
            
        except Exception as e:
            logger.error(f"âŒ Erreur pipeline CSV: {e}")
            raise
    
    def analyze_csv_data_quality(self):
        """ğŸ” Analyse la qualitÃ© des donnÃ©es CSV enrichies"""
        logger.info("ğŸ” Analyse de la qualitÃ© des donnÃ©es CSV...")
        
        if self.enriched_data is None:
            return
        
        # Analyse des valeurs manquantes
        missing_analysis = self.enriched_data.isnull().sum()
        missing_pct = (missing_analysis / len(self.enriched_data)) * 100
        
        # Features avec beaucoup de valeurs manquantes
        problematic_features = missing_pct[missing_pct > 20].sort_values(ascending=False)
        
        if len(problematic_features) > 0:
            logger.warning(f"âš ï¸ Features CSV avec >20% valeurs manquantes:")
            for feature, pct in problematic_features.items():
                logger.warning(f"   {feature}: {pct:.1f}%")
        
        # Analyse par pays
        countries_data_count = self.enriched_data['country_name'].value_counts()
        logger.info(f"ğŸ“Š Top 5 pays avec le plus de donnÃ©es: {countries_data_count.head().to_dict()}")
        logger.info(f"ğŸ“Š Pays avec le moins de donnÃ©es: {countries_data_count.tail().to_dict()}")
        
        # VÃ©rifier la cohÃ©rence des dates
        date_range = self.enriched_data['date'].max() - self.enriched_data['date'].min()
        logger.info(f"ğŸ“… Ã‰tendue temporelle: {date_range.days} jours")
        
        # Sauvegarde du rapport CSV
        self.save_csv_data_quality_report(missing_pct, countries_data_count)
    
    def save_csv_data_quality_report(self, missing_pct, countries_data_count):
        """ğŸ’¾ Sauvegarde un rapport de qualitÃ© des donnÃ©es CSV"""
        try:
            # Graphiques de qualitÃ© CSV
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # 1. Valeurs manquantes
            top_missing = missing_pct[missing_pct > 0].head(15)
            if len(top_missing) > 0:
                axes[0,0].barh(range(len(top_missing)), top_missing.values)
                axes[0,0].set_yticks(range(len(top_missing)))
                axes[0,0].set_yticklabels(top_missing.index, fontsize=8)
                axes[0,0].set_xlabel('Pourcentage de valeurs manquantes')
                axes[0,0].set_title('Top 15 Features CSV avec Valeurs Manquantes')
            
            # 2. Distribution des donnÃ©es par pays
            top_countries = countries_data_count.head(10)
            axes[0,1].bar(range(len(top_countries)), top_countries.values)
            axes[0,1].set_xticks(range(len(top_countries)))
            axes[0,1].set_xticklabels(top_countries.index, rotation=45, fontsize=8)
            axes[0,1].set_ylabel('Nombre de points de donnÃ©es')
            axes[0,1].set_title('Top 10 Pays par Volume de DonnÃ©es CSV')
            
            # 3. Ã‰volution temporelle
            temporal_data = self.enriched_data.groupby('date').size()
            axes[1,0].plot(temporal_data.index, temporal_data.values)
            axes[1,0].set_xlabel('Date')
            axes[1,0].set_ylabel('Nombre de points de donnÃ©es')
            axes[1,0].set_title('Ã‰volution Temporelle des DonnÃ©es CSV')
            axes[1,0].tick_params(axis='x', rotation=45)
            
            # 4. Distribution des valeurs COVID
            covid_columns = ['confirmed', 'deaths', 'recovered', 'active']
            available_covid = [col for col in covid_columns if col in self.enriched_data.columns]
            
            if available_covid:
                for i, col in enumerate(available_covid[:4]):
                    if col in self.enriched_data.columns:
                        # Log scale pour mieux voir la distribution
                        values = self.enriched_data[col][self.enriched_data[col] > 0]
                        if len(values) > 0:
                            axes[1,1].hist(np.log10(values + 1), alpha=0.5, label=col, bins=30)
                
                axes[1,1].set_xlabel('Log10(Valeur + 1)')
                axes[1,1].set_ylabel('FrÃ©quence')
                axes[1,1].set_title('Distribution des Valeurs COVID (Ã©chelle log)')
                axes[1,1].legend()
            
            plt.tight_layout()
            plt.savefig('outputs/csv_data_quality_report.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("âœ… Rapport qualitÃ© CSV sauvegardÃ©: outputs/csv_data_quality_report.png")
            
        except Exception as e:
            logger.error(f"âŒ Erreur sauvegarde rapport qualitÃ© CSV: {e}")
    
    def run_model_training(self):
        """ğŸ§  Lance l'entraÃ®nement du modÃ¨le rÃ©volutionnaire avec donnÃ©es CSV"""
        logger.info("ğŸ§  Ã‰TAPE 2: ENTRAÃNEMENT DU MODÃˆLE RÃ‰VOLUTIONNAIRE (CSV)")
        
        if self.enriched_data is None:
            raise ValueError("DonnÃ©es CSV enrichies non disponibles. Lancez d'abord le pipeline CSV.")
        
        # Configuration du modÃ¨le (identique Ã  la version MongoDB)
        model_config = self.config.get('model_config', {
            'd_model': 256,
            'n_heads': 8,
            'n_layers': 6,
            'd_ff': 1024,
            'dropout': 0.1,
            'prediction_horizons': [1, 7, 14, 30]
        })
        
        self.trainer = CovidRevolutionaryTrainer(model_config)
        
        try:
            # PrÃ©paration des donnÃ©es
            logger.info("ğŸ¯ PrÃ©paration des donnÃ©es CSV pour l'entraÃ®nement...")
            sequences, static_features, targets = self.trainer.prepare_revolutionary_dataset(
                self.enriched_data, 
                sequence_length=self.config.get('sequence_length', 30)
            )
            
            # CrÃ©ation des DataLoaders
            train_loader, val_loader = self.trainer.create_dataloaders(
                sequences, static_features, targets,
                batch_size=self.config.get('batch_size', 32),
                val_split=self.config.get('val_split', 0.2)
            )
            
            # EntraÃ®nement
            logger.info("ğŸš€ DÃ©marrage de l'entraÃ®nement rÃ©volutionnaire (donnÃ©es CSV)...")
            history = self.trainer.train_revolutionary_model(
                train_loader, val_loader,
                epochs=self.config.get('epochs', 100),
                learning_rate=self.config.get('learning_rate', 1e-4)
            )
            
            # Sauvegarde des artefacts
            self.trainer.save_artifacts(history)
            
            # Ã‰valuation finale
            self.evaluate_model_performance(val_loader, history)
            
            return history
            
        except Exception as e:
            logger.error(f"âŒ Erreur entraÃ®nement CSV: {e}")
            raise
    
    def evaluate_model_performance(self, val_loader, history):
        """ğŸ“Š Ã‰value les performances finales du modÃ¨le CSV"""
        logger.info("ğŸ“Š Ã‰VALUATION FINALE DU MODÃˆLE (DONNÃ‰ES CSV)")
        
        try:
            # MÃ©triques d'entraÃ®nement
            if history and 'val_metrics' in history:
                final_metrics = history['val_metrics'][-1] if history['val_metrics'] else {}
                
                logger.info("ğŸ¯ MÃ‰TRIQUES FINALES (CSV):")
                for metric_name, value in final_metrics.items():
                    if isinstance(value, (int, float)):
                        logger.info(f"   {metric_name}: {value:.4f}")
            
            # Test sur quelques prÃ©dictions
            self.test_csv_sample_predictions()
            
        except Exception as e:
            logger.error(f"âŒ Erreur Ã©valuation CSV: {e}")
    
    def test_csv_sample_predictions(self):
        """ğŸ§ª Teste le modÃ¨le sur quelques prÃ©dictions Ã©chantillons CSV"""
        logger.info("ğŸ§ª Test de prÃ©dictions Ã©chantillons (donnÃ©es CSV)...")
        
        try:
            # SÃ©lectionner quelques pays pour test
            test_countries = ['France', 'Germany', 'Italy', 'Spain', 'United Kingdom']
            available_countries = self.enriched_data['country_name'].unique()
            
            test_countries = [c for c in test_countries if c in available_countries][:3]
            
            if not test_countries:
                test_countries = list(available_countries)[:3]
            
            logger.info(f"ğŸ§ª Test CSV sur les pays: {test_countries}")
            
            # Simulations de prÃ©dictions (logique simplifiÃ©e)
            for country in test_countries:
                country_data = self.enriched_data[self.enriched_data['country_name'] == country]
                if len(country_data) > 30:
                    latest_data = country_data.tail(1).iloc[0]
                    logger.info(f"   {country}: DerniÃ¨res donnÃ©es CSV - "
                              f"ConfirmÃ©s: {latest_data.get('confirmed', 0):,.0f}, "
                              f"DÃ©cÃ¨s: {latest_data.get('deaths', 0):,.0f}")
        
        except Exception as e:
            logger.error(f"âŒ Erreur test prÃ©dictions CSV: {e}")
    
    def generate_csv_final_report(self, history):
        """ğŸ“ GÃ©nÃ¨re un rapport final CSV complet"""
        logger.info("ğŸ“ GÃ‰NÃ‰RATION DU RAPPORT FINAL CSV")
        
        try:
            report = {
                "training_summary": {
                    "model_type": "COVID Revolutionary Transformer v2.0 (CSV)",
                    "data_source": "Pure CSV Pipeline",
                    "training_date": datetime.now().isoformat(),
                    "dataset_size": len(self.enriched_data) if self.enriched_data is not None else 0,
                    "countries_count": self.enriched_data['country_name'].nunique() if self.enriched_data is not None else 0,
                    "features_count": len(self.enriched_data.columns) if self.enriched_data is not None else 0,
                    "epochs_completed": len(history.get('train_loss', [])) if history else 0
                },
                "csv_data_sources": {
                    "covid_timeseries": "covid_19_clean_complete_clean.csv / full_grouped_clean.csv",
                    "vaccination_data": "cumulative-covid-vaccinations_clean.csv",
                    "demographics": "consolidated_demographics_data.csv"
                },
                "model_architecture": self.config.get('model_config', {}),
                "training_config": {
                    k: v for k, v in self.config.items() 
                    if k not in ['csv_data_path']
                },
                "final_performance": history.get('val_metrics', [])[-1] if history and history.get('val_metrics') else {}
            }
            
            # Sauvegarde du rapport
            import json
            with open('outputs/csv_final_training_report.json', 'w') as f:
                json.dump(report, f, indent=2, default=str)
            
            logger.info("âœ… Rapport final CSV sauvegardÃ©: outputs/csv_final_training_report.json")
            
            # Affichage du rÃ©sumÃ©
            logger.info("ğŸ‰ RÃ‰SUMÃ‰ DE L'ENTRAÃNEMENT RÃ‰VOLUTIONNAIRE CSV:")
            logger.info(f"   ğŸ“Š Dataset CSV: {report['training_summary']['dataset_size']:,} lignes")
            logger.info(f"   ğŸ³ï¸ Pays: {report['training_summary']['countries_count']}")
            logger.info(f"   ğŸ“ˆ Features: {report['training_summary']['features_count']}")
            logger.info(f"   ğŸ”„ Epochs: {report['training_summary']['epochs_completed']}")
            
            if report['final_performance']:
                logger.info("   ğŸ¯ Performance finale CSV:")
                for metric, value in report['final_performance'].items():
                    if isinstance(value, (int, float)):
                        logger.info(f"      {metric}: {value:.4f}")
            
        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©ration rapport CSV: {e}")
    
    def run_complete_csv_training(self):
        """ğŸš€ Lance l'entraÃ®nement complet rÃ©volutionnaire CSV"""
        logger.info("ğŸš€ DÃ‰MARRAGE DE L'ENTRAÃNEMENT RÃ‰VOLUTIONNAIRE CSV COMPLET")
        logger.info("=" * 80)
        
        try:
            # 1. Validation environnement CSV
            if not self.validate_csv_environment():
                raise ValueError("Environnement CSV non valide")
            
            # 2. Pipeline de donnÃ©es CSV
            self.run_csv_data_pipeline()
            
            # 3. EntraÃ®nement du modÃ¨le
            history = self.run_model_training()
            
            # 4. Rapport final CSV
            self.generate_csv_final_report(history)
            
            logger.info("=" * 80)
            logger.info("ğŸ‰ ENTRAÃNEMENT RÃ‰VOLUTIONNAIRE CSV TERMINÃ‰ AVEC SUCCÃˆS!")
            logger.info("ğŸ“ Fichiers gÃ©nÃ©rÃ©s:")
            logger.info("   - models/covid_revolutionary_model.pth")
            logger.info("   - models/revolutionary_*_scaler.pkl")
            logger.info("   - models/revolutionary_config.json")
            logger.info("   - outputs/csv_data_quality_report.png")
            logger.info("   - outputs/csv_final_training_report.json")
            logger.info("   - models/training_history.png")
            logger.info("\nğŸš€ Le modÃ¨le rÃ©volutionnaire CSV est prÃªt pour l'API!")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ã‰CHEC DE L'ENTRAÃNEMENT CSV: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ğŸš€ Point d'entrÃ©e principal CSV"""
    parser = argparse.ArgumentParser(description="EntraÃ®nement rÃ©volutionnaire COVID IA v2.0 - Version CSV")
    parser.add_argument("--config", type=str, help="Fichier de configuration JSON")
    parser.add_argument("--csv-path", type=str, default="../data/dataset_clean", help="Chemin des fichiers CSV")
    parser.add_argument("--epochs", type=int, default=100, help="Nombre d'epochs")
    parser.add_argument("--batch-size", type=int, default=32, help="Taille du batch")
    parser.add_argument("--learning-rate", type=float, default=1e-4, help="Taux d'apprentissage")
    
    args = parser.parse_args()
    
    # Configuration CSV par dÃ©faut
    config = {
        'csv_data_path': args.csv_path,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'sequence_length': 30,
        'val_split': 0.2,
        'model_config': {
            'd_model': 256,
            'n_heads': 8,
            'n_layers': 6,
            'd_ff': 1024,
            'dropout': 0.1,
            'prediction_horizons': [1, 7, 14, 30]
        }
    }
    
    # Charger configuration depuis fichier si spÃ©cifiÃ©
    if args.config and os.path.exists(args.config):
        import json
        with open(args.config, 'r') as f:
            file_config = json.load(f)
            config.update(file_config)
        logger.info(f"âœ… Configuration CSV chargÃ©e depuis {args.config}")
    
    # Lancer l'entraÃ®nement CSV
    orchestrator = CSVRevolutionaryTrainingOrchestrator(config)
    success = orchestrator.run_complete_csv_training()
    
    if success:
        print("\n" + "="*50)
        print("ğŸ‰ SUCCÃˆS! Le modÃ¨le rÃ©volutionnaire CSV est prÃªt!")
        print("ğŸ“š Prochaines Ã©tapes:")
        print("   1. Lancer l'API: python covid_revolutionary_api.py")
        print("   2. Tester les prÃ©dictions: /predict endpoint")
        print("   3. IntÃ©grer avec le dashboard Vue.js")
        print("="*50)
        sys.exit(0)
    else:
        print("\nâŒ Ã‰CHEC de l'entraÃ®nement CSV. VÃ©rifiez les logs.")
        sys.exit(1)

if __name__ == "__main__":
    main()